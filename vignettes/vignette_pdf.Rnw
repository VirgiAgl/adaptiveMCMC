\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage[round]{natbib}

% The line below tells R to use knitr on this.
%\VignetteEngine{knitr::knitr}

\title{OxWaSP Module 1: Adaptive MCMC}
\author{Virginia Aglietti \and Tamar Loach}

\begin{document}

\maketitle

\section{Introduction to adaptive MCMC}

MCMC algorithms allow sampling from complicated, high-dimensional distributions. Choice of the proposal distribution (from which samples are taken in an attempt to approximate sampling from the target distribution $\pi$) determines the ability of the algorithm to explore the parameter space fully and hence draw a good sample. Adaptive MCMC algorithms tackle this challenge by using samples already generated to learn about the target distribution; they push this knowledge back to the choice of proposal distribution iteratively.

This project explores adaptive MCMC algorithms existing in the literature that use covariance estimators to improve convergence to a target distribution supported on a subset of $\mathbb{R}^d$. In this schema we learn about the target distribution $\pi$ through estimation of its correlation structure from the MCMC samples. We use this correlation structure to improve our estimate of the target. The performance of the algorithm depends heavily on the choice of the proposal distribution and its covariance structure. Different choices for the proposal covariance matrix may lead to different results. On the one hand, if the proposal covariance matrix is too narrow, the parameter space won't be properly explored. On the other hand, if the proposal covariance matrix is too wide, the rejection rate may be very high. In order to obtain a chain that adapts properly and settles down to rapid mixing, we need to select an optimal value for the covariance matrix.  Roberts \emph{et al.} (1997) have first shown that, under specific assumptions about the target distribution, the optimal value for the proposed covariance matrix is such that the acceptance rate of the algorithm is $0.234$, independently of the d-dimensional target distribution with \emph{iid} components. An optimal acceptance rate $\alpha^{*}=0.234$ will be used to compare the performance of the algorithms discussed in this report.

\section{The AM algorithm}

We first implement an adaptive MCMC algorithm which we will here call AM \citep{haario2001}. This is a modification of the random walk Metropolis-Hastings algorithm. In the AM algorithm the proposal distribution is updated at time $t$ to be a normal distribution centered on the current point $X_{t-1}$ with covariance $C_t(X_0, ..., X_{t-1})$ that depends on the the whole history of the chain. The use of historic states means the resulting chain is non-markovian, and reversibility conditions are not satisfied. Haario \emph{et al.} show that the right ergodic properties and correct simulation of the target distribution nonetheless remain. Provided we use a asymptotically symmetric proposal distribution the probability with which to accept candidate points in the chain is:


\begin{align}
\alpha(X_{t-1},Y) = \text{min}\left( 1,\frac{\pi(Y)}{\pi(X_{t-1})}\right).
\end{align}

With $C_t$ given by:

\begin{align}
C_t = s_d \text{cov}(X_0, ..., X_{t-1}) + s_d\epsilon I_d.
\end{align}

Here $\text{cov}()$ is the usual empirical covariance matrix:

\begin{align}
\text{cov}(x_0,...,x_k)=\frac{1}{k} \left( \Sigma_{i=0}^{k}x_ix_i^T-(k+1)\bar{x}_k\bar{x}_k^T \right),
\end{align}

and the parameter $s_d = \frac{2.4^2}{d}$ \citep{gelman1996}. $\epsilon$ is chosen to be very small compared to the subset of $\mathbb{R}^d$ upon which the target function is supported. The AM algorithm is computationally feasible due to recursive updating of the covariance matrix on acquisition of each new sample through the relation:

\begin{align}
C_{t+1} = \frac{t-1}{t} C_t + \frac{s_d}{t}(t \bar{X}_{t-1}\bar{X}^T_{t-1} - (t+1)\bar{X}_t \bar{X}^T_t + X_tX_t^T + \epsilon I_d).
\end{align}

The mean is in turns calculated recursively by:

\begin{align}
\bar{X}_{t+1} = \frac{t \bar{X}_{t}  + X_{t+1}}{t+1}.
\end{align}

Because of the instability of the covariance matrix, to implement the adaptivity we first run the algorithm with no change to the covariance of the proposal distribution. The adaptation starts at a user defined point in time, and until this time the covariance of the proposal is chosen to represent our best knowledge of the target distribution. We use a Gaussian distribution with no correlation structure thorughout this report.


\section{An example - testing the AM algorithm}

We now numerically test the AM algorithm. We have used two different target distributions: a correlated Gaussian distribution $N(0,\Sigma)$ and a banana-shaped distribution \citep{roberts2009} given by:

\begin{align}
f_B\left(x_1,...,x_d\right)\propto \exp \left[ -x_1^2/200 - \frac{1}{2} \left(x_2 + Bx_1^2-100B\right)^2 - \frac{1}{2} \left(x_3^2 + x_4^2 + ... + x_d^2\right) \right].
\end{align}

$B > 0$ is the so called bananicty constant (set to 0.1 throughout) and $d$ is the dimension. We have chosen the correlated Gaussian distribution as targeting this demonstrates how the use of empirical covariance improves convergence - we learn the target's covariance as we move through steps of the MCMC. The banana-shaped distribution is an additional example with an irregular shape. We use this to test the ability of the markov chain to fully explore the state space with and without adaption. We first run our implementation of the usual random-walk Metropolis-Hasting algorithm, and the AM adaptation modification of this, each time targetting $N(0,\Sigma)$. We have chosen $\Sigma$ to be generated from eigenvalues chosen unifromly at random on [1,10].

The crucial function in the R package that we have created is called \texttt{mcmc()}. This function takes as arguments the following parameters:

<<initialisations, eval=FALSE>>=

d = 8                             #  Dimension of the parameter space
n_iter = 20000                    #  Total number of iterations
x1 = rep(5,d)                     #  Vector of inital values
t_adapt = 2000                    #  When to start adapting
adapt = "AM"                      #  Algorithm to run.
target = pi_norm_corr             #  Target distribution function
cov_estimator="Sample covariance" # Coviance matrix estimator
@

There are several target distribution functions built in to our package for testing the algorithm. Notice that \texttt{x1} represents the starting point of the chain and is user specified. We show here the call to the function \texttt{mcmc()} using as target distribution a correlated Gaussian distribution:

<<initialisations2, eval=FALSE>>=

X = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x1,
         adapt=adapt,
         t_adapt = t_adapt
         )
@



\section{Comparing AM and random-walk Metropolis-Hastings}

Figure \ref{fig:mh_on_correlated_Gaussian_plots} shows the first component and the second component of the Markov chain at each iteration. The plot demostrates poor mixing of the MH algorithm when targeting a positively correlated multivariate Gaussian distribution (Figure \ref{fig:mh_on_correlated_Gaussian_plots2}). %positive or negative look at the second plot%.

<<mh_on_correlated_Gaussian_plots,out.width='0.7\\linewidth',out.height='0.8\\linewidth',fig.align="center",fig.cap="The first (top) and second (bottom) components of the Markov chain resulting from Metropolis-Hastings (MH) targetting a correlated 8-dimensional Gaussian distribution.", echo=FALSE>>=

data_path = "/data/toucanet/aglietti/adaptiveMCMC/adaptiveMCMC_paralellisation/data_24_10/"

load(file=paste(data_path,'X_AM_none.rda',sep=""))

plot1=plotIterations(
                    X_AM_none$X[,1],
                    n_iter,
                    title="1st component"
                    )
plot2=plotIterations(
                    X_AM_none$X[,2],
                    n_iter,
                    title="2nd component"
                    )
grid.arrange(plot1, plot2, nrow=2)

@


<<mh_on_correlated_Gaussian_plots2,out.width='0.7\\linewidth',out.height='0.4\\linewidth',fig.align="center",fig.cap="The relationship between the first two components of the Markov chain generated using Metropolis-Hasting targeting the correlated Gaussian distribution.", echo=FALSE>>=

plot3=plotComponents(
                    X_AM_none$X[,1],
                    X_AM_none$X[,2],
                    Xtitle = "1st component",
                    Ytitle = "2nd component"
                    )
plot3
@

Figure \ref{fig:am_on_correlated_Gaussian} shows the same results obtained from implementing the AM algorithm. The AM algorithm seems to perform better in terms of parameter space exploration and appears to settle down to a rapid mixing. In addition it seems to better capture the correlation existing among the variables (Figure \ref{fig:am_on_correlated_Gaussian2}).

<<am_on_correlated_Gaussian,out.width='0.7\\linewidth',out.height='0.8\\linewidth',fig.align="center",fig.cap="The first (top) and second (bottom) components of the Markov chain resulting from the AM algorithm targetting a correlated 8-dimensional Gaussian distribution.", echo=FALSE>>=

data_path = "/data/toucanet/aglietti/adaptiveMCMC/adaptiveMCMC_paralellisation/data_24_10/"

load(file=paste(data_path,'X_AM_sample.rda',sep=""))

plot6=plotIterations(
                    X_AM_sample$X[,1],
                    n_iter,
                    title="1st component"
                    )
plot7=plotIterations(
                    X_AM_sample$X[,2],
                    n_iter,
                    title="2nd component"
                    )
grid.arrange(plot6, plot7, nrow=2)
@

<<am_on_correlated_Gaussian2,out.width='0.7\\linewidth',out.height='0.4\\linewidth',fig.align="center",fig.cap="The relationship between the first two components of the Markov chain resulting from the adaptive metropolis algorithm (AM) targeting a correlated 8-dimensional Gaussian distribution.", echo=FALSE>>=

plot8=plotComponents(
                    X_AM_sample$X[,1],
                    X_AM_sample$X[,2],
                    Xtitle = "1st component",
                    Ytitle = "2nd component"
                    )
plot8
@


We now repeat this analysis using a multidimensional banana-shape distribution as target distribution. Indeed, the AM algorithm is expected to work particularly well on irregularly shaped target densities in which the density contours form roughly elliptical contours. Figure \ref{fig:mh_am_banana} proves rapid mixing of the AM algorithm with respect to the MH algorithm.

<<mh_am_banana,out.width='0.7\\linewidth',out.height='0.8\\linewidth',fig.align="center",fig.cap="The first and second components of the samples resulting from MH algorithm (left) and AM algorithm (right) targeting an 8-dimensional banana shaped distribution.", echo=FALSE>>=

data_path = "/data/toucanet/aglietti/adaptiveMCMC/adaptiveMCMC_paralellisation/data_24_10/"

load(file=paste(data_path,'X_AM_none_pi_banana8.rda',sep=""))
load(file=paste(data_path,'X_AM_sample_pi_banana8.rda',sep=""))

plot11=plotComponents(
                    X_AM_none_pi_banana8$X[,2],
                    X_AM_none_pi_banana8$X[,1],
                    Xtitle = "1st component",
                    Ytitle = "2nd component"
                    )

plot12=plotComponents(
                    X_AM_sample_pi_banana8$X[,2],
                    X_AM_sample_pi_banana8$X[,1],
                    Xtitle = "1st component",
                    Ytitle = "2nd component"
                    )
grid.arrange(plot11, plot12, ncol=2)


@


\section{Comparing AM and AM2}

We now explore a slight modification to the adaptation scheme which we will here call AM2 \citep{roberts2009}. This algorithm uses stochastic stabilisation rather than the numerical stabilisation of AM. Roberts and Rosenthal use a mixture of Gaussians as the proposal distribution: with proportion $\beta$ a normal uncorrelated distribution is mixed with a correlated normal distribution. The proposal becomes:

\begin{align}
Q_n(x,\cdot) = (1-\beta) N(x,s_d \Sigma_n) + \beta (N(x,(0.1^2)I_d/d)
\end{align}

Our implementation (AM2) is otherwise the same as AM1. We choose to leave the choice of the time at which the adaptation is introduced to the user; Roberts and Rosenthal specify the adaptation time to be when there have been more than $2d$ iterations but we find better performance with a longer period of standard random walk Metropolis-Hastings first. The results for our implementation of AM2 are as follows.


\section{Less naive covariance estimation}

The usual empirical covariance matrix estimator used so far in the adaptive AM and AM2 algorithms is optimal in the classical setting with large samples and fixed low dimensions. This estimator performs poorly in the high dimensional setting and in particular when the dimension of the parameter space is larger than the number of observations. In the recent literature, several alternative covariance matrix estimation techniques have been proposed. In this paper we will focus on two different regularization techniques: the shrinkage estimator and a Cholesky-based method.

Define the empirical covariance matrix as in (3) and a target identity matrix as $D=\text{diag}(d)$ where $d$ represents the dimension of the problem. The idea of shrinkage estimation of the covariance matrix is to take a weighted average of the empirical covariance matrix and $D$ given a parameter $\lambda$ representing the shrinkage intensity - see Schafer \emph{et al.} \citep{schafer2005shrinkage} for its computation.The covariance matrix used in the algorithm can be defined as:

\begin{align}
\hat{C} = (1-\lambda) \hat{\Sigma} + \lambda*\hat{D}.
\end{align}

Notice that $0 < \lambda< 1$. When $\lambda=0$, no shrinkage is applied to the sample covariance matrix and the empirical covariance matrix is used. In contrast $\lambda=1$ is associated with complete shrinkage of all pairwise covariances. In this case, we are thus ignoring the existence of any covariances among the random variables considered.
It is possible to show that, under general conditions, there exists a shrinkage intensity for which the resulting shrinkage estimator contains less estimation error than the original empirical estimator \citep{james1961estimation}.

As an alternative to both the empirical and the shrinkage estimator of the covariance matrix, we have used a regularization techniques based on the Cholesky decomposition of the covariance matrix \citep{rothman2010new}. Consider covariance matrix given by $\Sigma$. Define $\Sigma=LDL^T$ to be the modified Cholesky decomposition of the covariance matrix where $D$ is diagonal and $L$ is lower triangular with ones on the diagonal. The Cholesky-based method is based on the idea of bounding the Cholesky factor $L$. This means introducing sparsity in the Cholesky factor $L$ estimating only the first $k$ sub diagonal and setting the rest to zero.
The bounding parameter $k$ must be less than $\text{min(n-1,p)}$. Notice that a similar approach for bounding the inverse of the covariance matrix has been proposed by \citep{bickel2008regularized}. However, Rothman \emph{et al.} have shown that the Cholesky based regularization method can be applied directly to the covariance matrix itself to obtain a sparse estimator with guaranteed positive definiteness \citep{rothman2010new}.

In order to reduce the computational cost of the algorithm, the shrinkage estimator of the covariance structure has been computed using a recursion formula similar to (4).
On the contrary, the Cholesky-based covariance estimator has been evaluated at each step. Whilst this negatively impacts on the running-time of the algorithm, it doesn't affect its performance in terms of convergence rate $\alpha$.


The following plot shows $\alpha$ for all the discussed algorithms. We can see how the acceptance rate is far away from the optimum for the MH, AM and AM2 algorithms. Only when we introduce less naive covariance matrix estimators do we achieve better acceptance rates (Figure \ref{fig:am1_am2_banana_diffCov} and Figure \ref{fig:am1_am2_diffCov}).


<<am1_am2_diffCov,out.width='0.9\\linewidth',out.height='0.9\\linewidth',fig.align="center",fig.cap="The acceptance probability for the implemented algorithms with a 8-dimensional correlated Gaussian target distribution. The optimal acceptance probability is also shown.", echo=FALSE>>=

data_path = "/data/toucanet/aglietti/adaptiveMCMC/adaptiveMCMC_paralellisation/data_24_10/"

load(file=paste(data_path,'X_AM2_sample.rda',sep=""))
load(file=paste(data_path,'X_AM_sh1410.rda',sep=""))
load(file=paste(data_path,'X_AM_TH.rda',sep=""))
#load(file=paste(data_path,'X_AM2_SHv.rda',sep="")) 
load(file=paste(data_path,'X_AM2_SH.rda',sep=""))


iteration=seq(from= 1, to=n_iter+1, by=1)


df1<-data.frame(iterations=iteration,acceptance_probability=X_AM_none$acceptance_rates)
df2<-data.frame(iterations=iteration,acceptance_probability=X_AM_sample$acceptance_rates)
df3<-data.frame(iterations=iteration,acceptance_probability=X_AM2_sample$acceptance_rates)
df4<-data.frame(iterations=iteration,acceptance_probability=X_AM_sh1410$acceptance_rates)
df5<-data.frame(iterations=iteration,acceptance_probability=X_AM2_SH$acceptance_rates)
df6<-data.frame(iterations=iteration,acceptance_probability=X_AM_TH$acceptance_rates)
#df7<-data.frame(iterations=iteration,acceptance_probability=X_AM2_SHv$acceptance_rates)
df8<-data.frame(iterations=iteration,acceptance_probability=rep(0.234,n_iter+1))

ggplot(df1,aes(iteration,acceptance_probability))+geom_line(aes(color="MH"))+
      geom_line(data=df2,aes(color="AM"))+
      geom_line(data=df3,aes(color="AM2"))+
      geom_line(data=df4,aes(color="AM + shrinkage"))+
      geom_line(data=df5,aes(color="AM2 + shrinkage"))+
      geom_line(data=df6,aes(color="AM + thresholding"))+
      #geom_line(data=df7,aes(color="AM2 + thresholding"))+
      geom_line(data=df8,aes(color="Optimal acceptance"))+
      labs(color="Alorithm:")

@


<<am1_am2_banana_diffCov,out.width='0.9\\linewidth',out.height='0.9\\linewidth',fig.align="center",fig.cap="The acceptance probability for the implemented algorithms with a multidimensional banana shaped target distribution. ", echo=FALSE>>=


data_path = "/data/toucanet/aglietti/adaptiveMCMC/adaptiveMCMC_paralellisation/data_24_10/"

load(file=paste(data_path,'X_AM2_sample_pi_banana8.rda',sep=""))
load(file=paste(data_path,'X_AM_sh1410_pi_banana8.rda',sep=""))
load(file=paste(data_path,'X_AM_TH_pi_banana8.rda',sep=""))
load(file=paste(data_path,'X_AM2_sample_pi_banana8.rda',sep="")) 
load(file=paste(data_path,'X_AM2_SH_pi_banana8.rda',sep=""))
load(file=paste(data_path,'X_AM2_TH_pi_banana8.rda',sep=""))



iteration=seq(from= 1, to=n_iter+1, by=1)

df1<-data.frame(iterations=iteration,acceptance_probability=X_AM_none_pi_banana8$acceptance_rates)
df2<-data.frame(iterations=iteration,acceptance_probability=X_AM_sample_pi_banana8$acceptance_rates)
df3<-data.frame(iterations=iteration,acceptance_probability=X_AM2_sample_pi_banana8$acceptance_rates)
df4<-data.frame(iterations=iteration,acceptance_probability=X_AM_sh1410_pi_banana8$acceptance_rates)
df5<-data.frame(iterations=iteration,acceptance_probability=X_AM2_SH_pi_banana8$acceptance_rates)
df6<-data.frame(iterations=iteration,acceptance_probability=X_AM_TH_pi_banana8$acceptance_rates)
df7<-data.frame(iterations=iteration,acceptance_probability=X_AM2_TH_pi_banana8$acceptance_rates)
df8<-data.frame(iterations=iteration,acceptance_probability=rep(0.234,n_iter+1))

ggplot(df1,aes(iteration,acceptance_probability))+geom_line(aes(color="MH"))+
      geom_line(data=df2,aes(color="AM"))+
      geom_line(data=df3,aes(color="AM2"))+
      geom_line(data=df4,aes(color="AM + shrinkage"))+
      geom_line(data=df5,aes(color="AM2 + shrinkage"))+
      geom_line(data=df6,aes(color="AM + thresholding"))+
      geom_line(data=df7,aes(color="AM2 + thresholding"))+
      geom_line(data=df8,aes(color="Optimal acceptance"))+
      labs(color="Alorithm:")

@

\section{Conclusions}

We have shown that the AM adaptive algorithm and the AM2 adaptive algorithm perform better than the simple random-walk Metropolis-Hastings algorithm. This is true for irregularly shaped distributions which are more challenging to sample from.


% now generate the bibliography from file adaptiveMCMC.bib
\bibliographystyle{plainnat}
\bibliography{adaptiveMCMC}

\end{document}
