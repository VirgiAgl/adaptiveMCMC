\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
\usepackage[round]{natbib}

% The line below tells R to use knitr on this.
%\VignetteEngine{knitr::knitr}

\title{OxWaSP Module 1: Adaptive MCMC}
\author{Virginia Aglietti \and Tamar Loach}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle

\section{Introduction to adaptive MCMC - the AM algorithm}

MCMC algorithms allow sampling from complicated, high-dimensional distributions. Choice of the proposal distribution (from which samples are taken in an attempt to approximate sampling from the target distribution $\pi$) determines the ability of the algorithm to explore the parameter space fully and hence draw a good sample. Adaptive MCMC algorithms tackle this challenge by using samples already generated to learn about the target distribution; they push this knowledge back to the choice of proposal distribution iteratively.

This project explores adaptive MCMC algorithms existing in the literature that use covariance estimators to improve convergence to a target distribution supported on a subset of $\mathbb{R}^d$. In this schema we learn about the target distribution $\pi$ through estimation of its correlation structure from the MCMC samples. We use this correlation structure to improve our estimate of the target.

We first implement an adaptive MCMC algorithm AM \citep{haario2001} which is a modification of the random walk Metropolis-Hastings algorithm. In AM the proposal distribution is updated at time $t$ to be a normal distribution centered on the current point $X_{t-1}$ with covariance $C_t(X_0, ..., X_{t-1})$ that depends on the the whole history of the chain. The use of historic states means the resulting chain is non-markovian, and reversibility conditions are not satisfied. Haario et al show that, with a small update to the usual Metropolis-Hastings acceptance probability, the right ergodic properties and correct simulation of the target distribution none the less remain. The probability with which to accept candidate points in the chain becomes:

$$
\alpha(X_{t-1},Y) = \text{min}\left( 1,\frac{\pi(Y)}{\pi(X_{t-1})}\right)
$$

With $C_t$ given by:

$$
C_t = s_d \text{cov}(X_0, ..., X_{t-1}) + s_d\epsilon I_d
$$

Here $\text{cov}()$ is the usual empirical covariance matrix, and the parameter $s_d = \frac{2.4^2}{d}$ \citep{gelman1996}. $\epsilon$ is chosen to be very small compared to the subset of $\mathbb{R}^d$ upon which the target function is supported. The AM algorithm is computationally feasible %todo Tamar What is the computional compplexity?
due to recursive updating of the covariance matrix on acquisition of each new sample through the relation:

$$
C_{t+1} = \frac{t-1}{t} C_t + \frac{s_d}{t}(t \bar{X}_{t-1}\bar{X}^T_{t-1} - (t+1)\bar{X}_t \bar{X}^T_t + X_tX_t^T + \epsilon I_d)
$$
 with the mean calculated recursively by:
$$
\bar{X}_{t+1} = \frac{t \bar{X}_{t}  + X_{t+1}}{t+1}
$$

Because of the instability of the covariance matrix, to implement the adaptivity we first run the algorithm with no change to the covariance of the proposal distribution. The adaptation starts at a user defined point in time, and until this time the covariance of the proposal is chosen to represent our best knowledge of the target distribution.

\section{An example - testing the AM algorithm}

We now numerically test the AM algorithm. We have used two different target distributions: a correlated Gaussian distribution $N(0,\Sigma)$ and a "banana"-shaped distribution (\citep{roberts2009}) given by:

$$
f_B\left(x_1,...,x_d\right)\propto \exp \left[ -x_1^2/200 - \frac{1}{2} \left(x_2 + Bx_1^2-100B\right)^2 - \frac{1}{2} \left(x_3^2 + x_4^2 + ... + x_d^2\right) \right]
$$

$B > 0$ is the "bananicty" constant (set to 0.1 throughout) and $d$ is the dimension. We have chosen the correlated Guassian distribution as targetting this demonstrates how the use of empirical covariance improves convergence - we learn the target's covariance as we move through steps of the MCMC. The banana-shaped distribution is an additional example with an irregular shape. We use this to test the ability of the markov chain to fully explore the state space with and without adaption. We first run our implementation of the usual Metropolis-Hasting algorithm, and the AM adaptation modification of this, each time targetting $N(0,\Sigma)$. We have chosen $\Sigma$ to be generated from eigenvalues chosen unifromly at random on [1,10].


<<initialisations>>=

d = 8
n_iter = 200     #  Total number of iterations  = 100000 todo
x_1 = rep(5,d)      #  Vector of inital values
t_adapt = 50      #  When to start adapting = 10000 todo

@

<<mh_on_correlated_guassian_plots,out.width='0.7\\linewidth',out.height='0.5\\linewidth',fig.align="center",fig.cap="The first (top) and second (bottom) components of the markov chain resulting from Metropolis-Hastings (MH) targetting a correlated 8-dimensional Gaussian.", echo=FALSE>>=

load(file='/data/tinkerbird/loach/adaptiveMCMC/R/data/X_MH.rda')

plot1=plotIterations(
                    X_MH$X[,1],
                    n_iter,
                    title="1st component"
                    )
plot2=plotIterations(
                    X_MH$X[,2],
                    n_iter,
                    title="2nd component"
                    )
grid.arrange(plot1, plot2, nrow=2)

@


<<mh_on_correlated_guassian_plots2,out.width='0.7\\linewidth',out.height='0.25\\linewidth',fig.align="center",fig.cap="The relationship between the first two components of the markov chain generated using Metropolis-Hasting targetting the correlated Guassian.", echo=FALSE>>=

plot3=plotComponents(
                    X_MH$X[,1],
                    X_MH$X[,2],
                    Xtitle = "1st component",
                    Ytitle = "2nd component"
                    )
plot3
@

%todo do we need the graph of expectation from MH targetting the guassian?

<<mh_on_correlated_guassian_plots3,out.width='0.7\\linewidth',out.height='0.25\\linewidth',fig.align="center",fig.cap="The expectation calculated using the sample generated with Metropolis-Hasting targetting the correlated Guassian.", echo=FALSE>>=

plot5=plotIterations(X_MH$sample_mean[,1], n_iter, "Sample expectation")
plot5

@

<<am_on_correlated_guassian,out.width='0.7\\linewidth',out.height='0.5\\linewidth',fig.align="center",fig.cap="The first (top) and second (bottom) components of the markov chain resulting from the adaptive metropolis algorithm (AM) targetting a correlated 8-dimensional Gaussian."", echo=FALSE>>=

load(file='/data/tinkerbird/loach/adaptiveMCMC/R/data/X_AM.rda')

plot6=plotIterations(
                    X_AM$X[,1],
                    n_iter,
                    title="1st component"
                    )
plot7=plotIterations(
                    X_AM$X[,2],
                    n_iter,
                    title="2nd component"
                    )
grid.arrange(plot6, plot7, nrow=2)
@

<<am_on_correlated_guassian2,out.width='0.7\\linewidth',out.height='0.25\\linewidth',fig.align="center",fig.cap="The relationship between the first two components of the markov chain resulting from the adaptive metropolis algorithm (AM) targetting a correlated 8-dimensional Gaussian.", echo=FALSE>>=

plot8=plotComponents(
                    X_AM$X[,1],
                    X_AM$X[,2],
                    Xtitle = "1st component",
                    Ytitle = "2nd component"
                    )
plot8
@

<<am_on_correlated_guassian3,out.width='0.7\\linewidth',out.height='0.25\\linewidth',fig.align="center",fig.cap="The acceptance probability in the AM algorithm targetting the correlated Guassian.", echo=FALSE>>=

plot9=plotIterations(X_AM$acceptance_rates, n_iter, "Acceptance probability")
plot9

@

<<am_on_correlated_guassian4,out.width='0.7\\linewidth',out.height='0.25\\linewidth',fig.align="center",fig.cap=".", echo=FALSE>>=
plot10=plotIterations(X_AM$sample_mean[,1], n_iter, "Sample expection for the first component of MC")
plot10

@

AM is able to explore the state space, adapt properly and settle down to rapid mixing. We see an improved acceptance rate.

<<mh_am_banana>>=

x_1 = rep(5,2)      #  Vector of inital values
target = pi_banana


X_MH_banana = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="None"
         )

plot11=plotComponents(
                    X_MH_banana$X[,2],
                    X_MH_banana$X[,1],
                    Xtitle = "First component of the MC",
                    Ytitle = "Second component of the MC"
                    )

X_AM_banana = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         t_adapt = t_adapt,
         adapt="AM"
         )

plot12=plotComponents(
                    X_AM_banana$X[,2],
                    X_AM_banana$X[,1],
                    Xtitle = "First component of the MC",
                    Ytitle = "Second component of the MC"
                    )
grid.arrange(plot11, plot12, ncol=2)


@

We now explore a slight modification to the adaptation scheme AM2\citep{roberts2009}, that uses stochastic stabilisation rather than the numerical stabilisation of AM. Roberts and Rosenthal use a mixture of Gaussians as the proposal distribution: with proportion $\beta$ a normal uncorrelated distribution is mixed with a correlated normal distribution.

$$
Q_n(x,\cdot) = (1-\beta) N(x,s_d \Sigma_n) + \beta (N(x,(0.1^2)I_d/d)
$$

TODO Discuss the problems with starting dimension here.

%The results for our implementation of AM2 are as follows.

%We have used a higher dimensional normal distributionfor the proposal $20$ and the banana target $3$. Notice that the point at which we start adapting is determined by $d$, the dimension of the target distribution. The algorithm performs well when this $d$ is high; we believe this is due to the point at which the adaptation starts. We have thus approximated a 3-dimensional banana with a 20-dimensional normal gaussian distribution. This forces the adaptation to start later. We would like to further investigate this effect, and explore various dimensional spaces.



<<am1_am2_banana, fig.width=5, fig.cap="The first component of the 8-dimensional banana shaped distribution targetted using AM (left) and AM2 (right).">>=

x_1 = rep(5,8)      #  Vector of inital values
target = pi_banana8


X_AM2_banana = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM2"
         )


plot13=plotIterations(
                    X_AM_banana$X[,1],
                    n_iter,
                    title="Values of the first MC component"
                    )
plot14=plotIterations(
                    X_AM2_banana$X[,1],
                    n_iter,
                    title="Values of the first MC component"
                    )

grid.arrange(plot13, plot14, ncol=2)

@




<<am1_am2_banana_diffCov, fig.width=5 >>=
x_1 = rep(5,8)      #  Vector of inital values
cov_estimator1="Shrinkage estimator"
cov_estimator2="Thresholding estimator"
target = pi_banana8

X_MH_banana = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="None"
         )


X_AM_banana = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         t_adapt = t_adapt,
         adapt="AM"
         )

X_AM_banana_sh= mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM",
         t_adapt = t_adapt,
         cov_estimator=cov_estimator1
         )

X_AM2_banana_sh = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM2",
         cov_estimator=cov_estimator1
         )

X_AM_banana_th= mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM",
         t_adapt = t_adapt,
         cov_estimator=cov_estimator2
         )

X_AM2_banana_th = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM2",
         cov_estimator=cov_estimator2
         )

iteration=seq(from= 1, to=n_iter+1, by=1)
#data=data.frame(iteration_count=iteration, am_alpha=X_AM_banana_sh$acceptance_rates)


df1<-data.frame(iterations=iteration,acceptance_probability=X_MH_banana$acceptance_rates)
df2<-data.frame(iterations=iteration,acceptance_probability=X_AM_banana$acceptance_rates)
df3<-data.frame(iterations=iteration,acceptance_probability=X_AM2_banana$acceptance_rates)
df4<-data.frame(iterations=iteration,acceptance_probability=X_AM_banana_sh$acceptance_rates)
df5<-data.frame(iterations=iteration,acceptance_probability=X_AM2_banana_sh$acceptance_rates)
df6<-data.frame(iterations=iteration,acceptance_probability=X_AM_banana_th$acceptance_rates)
df7<-data.frame(iterations=iteration,acceptance_probability=X_AM2_banana_th$acceptance_rates)
df8<-data.frame(iterations=iteration,acceptance_probability=rep(0.234,n_iter+1))

ggplot(df1,aes(iterations,acceptance_probability))+geom_line(aes(color="MH"))+
      geom_line(data=df2,aes(color="AM"))+
      geom_line(data=df3,aes(color="AM2"))+
      geom_line(data=df4,aes(color="AM + shrinkage"))+
      geom_line(data=df5,aes(color="AM2 + shrinkage"))+
      geom_line(data=df6,aes(color="AM + thresholding"))+
      geom_line(data=df7,aes(color="AM2 + thresholding"))+
      geom_line(data=df8,aes(color="Optimal acceptance"))+
      labs(color="Alorithm:")

# all lines of the mean in the same plot with a legend

@



<<mean_diffCov, fig.width=5 >>=

target=pi_norm_corr


X_MH = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt=adapt)

X_AM = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt=adapt,
         t_adapt = t_adapt,
         cov_estimator=cov_estimator
         )
X_AM2 = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM2"
         )
X_AM_sh= mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM",
         t_adapt = t_adapt,
         cov_estimator=cov_estimator1
         )

X_AM2_sh = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM2",
         cov_estimator=cov_estimator1
         )

X_AM_th= mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM",
         t_adapt = t_adapt,
         cov_estimator=cov_estimator2
         )

X_AM2_th = mcmc(target = target,
         n_iter = n_iter,
         x_1 = x_1,
         adapt="AM2",
         cov_estimator=cov_estimator2
         )



df1<-data.frame(x=iteration,y=X_MH$sample_mean[,1])
df2<-data.frame(x=iteration,y=X_AM$sample_mean[,1])
df3<-data.frame(x=iteration,y=X_AM2$sample_mean[,1])
df4<-data.frame(x=iteration,y=X_AM_sh$sample_mean[,1])
df5<-data.frame(x=iteration,y=X_AM2_sh$sample_mean[,1])
df6<-data.frame(x=iteration,y=X_AM_th$sample_mean[,1])
df7<-data.frame(x=iteration,y=X_AM2_th$sample_mean[,1])
df8<-data.frame(x=iteration, y=rep(0,n_iter+1))

ggplot(df1,aes(x,y))+geom_line(aes(color="First line"))+
      geom_line(data=df2,aes(color="Second line"))+
      geom_line(data=df3,aes(color="Third line"))+
      geom_line(data=df4,aes(color="Forth line"))+
      geom_line(data=df5,aes(color="Fifth line"))+
      geom_line(data=df6,aes(color="Sixth line"))+
      geom_line(data=df7,aes(color="Seventh line"))+
      geom_line(data=df8,aes(color="Black"))+
      labs(color="Legend text")

# all lines of the mean in the same plot with a legend

@



\section{A Bit About knitr}

Whilst this demonstrates that the algorithms both approximately sample from the correct target distribution, there is no improvement in acceptance rate due to the adaptation steps. We require a less regular shaped target distribution to test whether adaptation improves our acceptance rate towards the optimal 0.234. For this we follow Haario et al in using a banana-shaped distribution. The following results show a significantly lower acceptance rate for the adaptive version of the algorithm.

The AM algorithm with banana target:

The MH algorithm with banana target:

We can see that the acceptance probability is closer to the optimal 0.234 with adaptation according to Haario et al.



\section{Less naive covariance estimation}

$\hat{C} = (1-\lambda) \hat{\Sigma} + \lambda*\hat{D}$

The usual empirical covariance matrix estimator used so far in the adaptive AM and AM2 algorithms is optimal in the classical setting with large samples and fixed low dimensions. This estimator performs poorly in the high dimensional setting and in particular when the dimension of the parameter space is larger than the number of observations. In the recent literature, several alternative covariance matrix estimation techniques have been proposed. In this paper we will focus on two different regularization techniques: the shrinkage estimator and the Cholesky-based method.

Define the empirical covariance matrix as:

%todo

where:

%todo

Define a target identity matrix as $D=\text{diag}(d)$ where $d$ represents the dimension of the problem. The idea of shrinkage estimation of the covariance matrix is to take a weighted average of the empirical covariance matrix and the target matrix given a parameter $\lambda$ representing the shrinkage intensity, (see Sch\aferand Strimmer (2005) for its computation ).The covariance matrix used in the algorithm can be defined as:


$\hat{C} = (1-\lambda) \hat{\Sigma} + \lambda*\hat{D}$

Notice that $0 < \lambda< 1$. When $\lambda=0$, no shrinkage is applied to the sample covariance matrix and the empirical covariance matrix is used. In contrast, $\lambda=1$ is associated to a complete shrinkage of all pairwise covariances. In this case, we are thus ignoring the existence of any covariances among the random variables considered.
It is possible to show that, under general conditions, there exists a shrinkage intensity for which the resulting shrinkage estimator contains less estimation error than the original empirical estimator; see James and Stein (1961). As an alternative to both the empirical and the shrinkage estimator of the covariance matrix, we have used a regularization techniques based on the Cholesky decomposition of the covariance matrix (ref). Consider a random vector X=() with covariance matrix given by $Sigma$.
Define sigma=formula to be the modified Cholesky decomposition of the covariance matrix where $D$ is diagonal and $L$ is lower triangular with one on the diagonal. The Cholesky-based method is based on the idea of bounding the Cholesky factor L that is introducing sparsity in the Cholesky factor L estimating only the first k sub diagonal and setting the rest to zero.
The bounding parameter k must be less than min(n-1,p). Notice that a similar approach for bounding the inverse of the covariance matrix has been proposed by (). However, (Authors ) have shown that the Cholesky based regularization method can be applied directly to the covariance matrix itself to obtain a sparse estimator with guaranteed positive definiteness.
In order to reduce the computational cost of the algorithm, the shrinkage estimator of the covariance structure has been computed using a recursion formula similar to (x). On the contrary, the Cholesky-based covariance estimator has been evaluated at each step. Whilst this negatively impact on the running-time of the algorithm, it doesn’t affect its performance in terms of convergence rate $\alpha$.

\section{\LaTeX}

\LaTeX itself is complicated if you've never used it before, but I'm sure you'll
pick it up quickly: there are a lot of guides on the web.  I recommend using the
\texttt{align} environment (in the \texttt{amsmath} package) for displayed equations:
\begin{align*}
% lose the *'s if you want equation numbers
f(x) &= x^3 - x - 1\\
g(y) &= y^4 + 2y
\end{align*}

You can cite in two ways using the \texttt{natbib} package:
\citep{haario2001}
and
\citet{haario2001}.

% now generate the bibliography from file adaptiveMCMC.bib
\bibliographystyle{plainnat}
\bibliography{adaptiveMCMC}

\end{document}
